{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #for data load\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences #for padding data\n",
    "from keras.utils import to_categorical # for categorizing data\n",
    "from sklearn.model_selection import train_test_split #for train and test spliting\n",
    "from keras.models import Sequential #for NN model\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed #for NN model\n",
    "from scienceie_loader import load_tokenized_data, load_data_with_char_offsets #for data preparing also customize dataset requirments for load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loading from data folder\n",
    "data_root = 'data'\n",
    "data_train = os.path.join(data_root, 'train2')\n",
    "data_test = os.path.join(data_root, 'semeval_articles_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting into train and test dataset and with customize function that comes with dataset\n",
    "# and this function define in scienceie_loader.py\n",
    "train_docs, train_rels, _ = load_tokenized_data(data_train)\n",
    "test_docs, test_rels, _ = load_tokenized_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feed into neural network we've to follow some for preparing data\n",
    "1. create unique words and labels\n",
    "2. separate and creating a new list with words and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence and labels:\n",
      "\n",
      "['RemarkThe', 'purely', 'radiative', 'spacetimes', 'used', 'as', 'reference', 'solutions', 'in', 'our', 'analysis', 'are', 'not', 'perturbations', 'of', 'the', 'Minkowski', 'spacetime.', 'A', 'way', 'of', 'seeing', 'this', 'is', 'to', 'consider', 'the', 'Newman–Penrose', 'constants', 'of', 'the', 'spacetime.', 'The', 'Newman–Penrose', 'constants', 'are', 'a', 'set', 'of', 'absolutely', 'conserved', 'quantities', 'defined', 'as', 'integrals', 'of', 'certain', 'components', 'of', 'the', 'Weyl', 'tensor', 'and', 'the', 'Maxwell', 'fields', 'over', 'cuts', 'of', 'null', 'infinity—see', '[', '19–21', ']', 'for', 'the', 'Einstein–Maxwell', 'case.', 'In', '[', '22', ']', 'it', 'has', 'been', 'shown', 'that', 'the', 'value', 'of', 'the', 'Newman–Penrose', 'constants', 'for', 'a', 'vacuum', 'radiative', 'spacetime', 'coincides', 'with', 'the', 'value', 'of', 'the', 'rescaled', 'Weyl', 'spinor', 'at', 'i+—this', 'result', 'can', 'be', 'extended', 'to', 'the', 'electrovacuum', 'case', 'using', 'the', 'methods', 'of', 'this', 'article.', 'For', 'the', 'radiative', 'spacetimes', 'arising', 'from', 'the', 'construction', 'of', '[', '17', ']', 'it', 'can', 'be', 'seen', 'that', 'the', 'value', 'of', 'the', 'Weyl', 'spinor', 'at', 'i+', 'is', 'essentially', 'the', 'mass', 'quadrupole', 'of', 'the', 'seed', 'static', 'spacetime.', 'It', 'follows', ',', 'that', 'the', 'Newman–Penrose', 'constants', 'of', 'the', 'radiative', 'spacetime', 'can', 'take', 'arbitrary', 'values.', 'On', 'the', 'other', 'hand', ',', 'for', 'the', 'Minkowski', 'spacetime', ',', 'the', 'Newman–Penrose', 'constants', 'are', 'exactly', 'zero', ',', 'and', 'those', 'of', 'perturbations', 'thereof', 'will', 'be', 'small.', 'Thus', ',', 'in', 'this', 'precise', 'sense', ',', 'our', 'radiative', 'spacetimes', 'are', ',', 'generically', ',', 'not', 'perturbations', 'of', 'the', 'Minkowski', 'spacetime', ',', 'unless', 'all', 'the', 'Newman–Penrose', 'constants', 'vanish', '.'] ['O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'B-Material', 'I-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Task', 'I-Task', 'O', 'O', 'O', 'O', 'B-Material', 'I-Material', 'O', 'O']\n",
      "\n",
      "\n",
      "The dataset contain:  350 Sentences\n",
      "The dataset contain:  350 Labels\n",
      "The dataset contain:  10052 Unique Token\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "tags = set()\n",
    "sens=[]\n",
    "labels=[]\n",
    "for doc in train_docs:#spiliting doc into a sent \n",
    "    s_tokens=[]\n",
    "    l_tokens=[]\n",
    "    for token, tag in doc: #spilitng sent into a token\n",
    "        words.add(token)\n",
    "        tags.add(tag)\n",
    "        s_tokens.append(token)\n",
    "        l_tokens.append(tag)\n",
    "    sens.append(s_tokens)\n",
    "    labels.append(l_tokens)\n",
    "\n",
    "#converting unique words and tags set() to list()\n",
    "words=list(words)\n",
    "tags=list(tags)    \n",
    "\n",
    "print(\"Sentence and labels:\\n\")\n",
    "print(sens[0], labels[0])\n",
    "print(\"\\n\")\n",
    "print(\"The dataset contain: \",len(sens), \"Sentences\")\n",
    "print(\"The dataset contain: \",len(labels), \"Labels\")\n",
    "print(\"The dataset contain: \",len(words), \"Unique Token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "3. creating dictonary with unique words and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary with unique words and labels\n",
    "word_dict = dict((c, i) for i, c in enumerate(words))\n",
    "label_dict = dict((c, i) for i, c in enumerate(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. following one hot encoding method encodeded all sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spiliting into sents/labels\n",
    "#spilitng into tokens\n",
    "#append token value accoring to word vocabulary\n",
    "#merging all tokens value in a new list\n",
    "sens_to_int=[]\n",
    "for i in sens:\n",
    "    s_token_to_int=[]\n",
    "    for j in i:\n",
    "        s_token_to_int.append(word_dict[j])\n",
    "    sens_to_int.append(s_token_to_int)\n",
    "\n",
    "labels_to_int=[]\n",
    "for i in labels:\n",
    "    l_token_to_int=[]\n",
    "    for j in i:\n",
    "        l_token_to_int.append(label_dict[j])\n",
    "    labels_to_int.append(l_token_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3431, 3273, 6427, 1348, 5143, 6317, 560, 6932, 7760, 3942, 5966, 3772, 5596, 2992, 7464, 9952, 2757, 9247, 6974, 5113, 7464, 4653, 6240, 1846, 7535, 416, 9952, 4255, 4115, 7464, 9952, 9247, 8539, 4255, 4115, 3772, 6585, 2159, 7464, 6590, 1099, 5724, 5652, 6317, 3560, 7464, 8887, 525, 7464, 9952, 4412, 5151, 8044, 9952, 5906, 6627, 8198, 4167, 7464, 3558, 9258, 9215, 4078, 6001, 8060, 9952, 4680, 2167, 3979, 9215, 5640, 6001, 7660, 1261, 1626, 4369, 3812, 9952, 8623, 7464, 9952, 4255, 4115, 8060, 6585, 3052, 6427, 1436, 4262, 4616, 9952, 8623, 7464, 9952, 7288, 4412, 5064, 3713, 444, 7933, 5856, 1944, 6713, 7535, 9952, 2653, 5534, 7875, 9952, 2881, 7464, 6240, 9410, 4643, 9952, 6427, 1348, 2063, 9354, 9952, 9043, 7464, 9215, 3741, 6001, 7660, 5856, 1944, 5659, 3812, 9952, 8623, 7464, 9952, 4412, 5064, 3713, 9483, 1846, 8854, 9952, 512, 6880, 7464, 9952, 7472, 1193, 9247, 5277, 5092, 3984, 3812, 9952, 4255, 4115, 7464, 9952, 6427, 1436, 5856, 9407, 2411, 9107, 4207, 9952, 1016, 3525, 3984, 8060, 9952, 2757, 1436, 3984, 9952, 4255, 4115, 3772, 509, 6101, 3984, 8044, 4786, 7464, 2992, 2236, 1240, 1944, 3940, 5887, 3984, 7760, 6240, 9571, 1987, 3984, 3942, 6427, 1348, 3772, 3984, 65, 3984, 5596, 2992, 7464, 9952, 2757, 1436, 3984, 9154, 91, 9952, 4255, 4115, 3515, 4105] [1, 1, 2, 6, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 5, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 5, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 1, 1, 5, 4, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#visualize form of sentences and labels after encoding\n",
    "print(sens_to_int[0], labels_to_int[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sens=max([len(i) for i in sens_to_int])#sent max length\n",
    "max_len_labels=max([len(i) for i in labels_to_int])#labels max length\n",
    "n_tags=len(tags)#number of tags in dataset\n",
    "n_words=len(words)#number of words in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Padding encoded sentences and labels with the maximum length of documents that confirmes they are in the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first parameter is the sequence\n",
    "#second parameter is the maximum length of sentence/label\n",
    "#third parameter is position of padding is \"post\" or \"pre\"\n",
    "#fouth parameter is the padding value\n",
    "X = pad_sequences(sens_to_int, max_len_sens, padding=\"post\",value=n_words - 1)\n",
    "Y = pad_sequences(labels_to_int, max_len_labels, padding=\"post\",value=label_dict[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Categorizing labels with number of tags containing in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [to_categorical(i, num_classes=n_tags) for i in Y]\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Spiliting dataset before feed into Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 318) (280, 318, 7)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train),np.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train),type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Making artchitecture of model and here we use LSTM as a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 318, 300)          3015600   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 318, 512)          1665024   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 318, 7)            3591      \n",
      "=================================================================\n",
      "Total params: 4,684,215\n",
      "Trainable params: 4,684,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#sequential class\n",
    "model = Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size,\n",
    "# output embedding dimension of size 300, and\n",
    "#input_length is a max length of sentences\n",
    "model.add(Embedding(input_dim=n_words, output_dim=300, input_length=max_len_sens))\n",
    "\n",
    "# LSTM layer with 512 internal memory units.\n",
    "#input shape will be maximum length of sentence with n_tags values\n",
    "model.add(LSTM(512, input_shape=(max_len_sens,n_tags), return_sequences=True))\n",
    "\n",
    "# Add a Dense layer with number of tags.\n",
    "#softmax activation function is used on the output to predict multinomial probability distribution.\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the multinomial log loss (categorical_crossentropy in Keras)\n",
    "#The efficient ADAM optimization algorithm is used to find the weights\n",
    "#the accuracy metric is calculated and reported each epoch\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Feeding data into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 1.6438 - accuracy: 0.5670 - val_loss: 0.9491 - val_accuracy: 0.8347\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.8157 - accuracy: 0.8399 - val_loss: 0.7124 - val_accuracy: 0.8347\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 33s 5s/step - loss: 0.7357 - accuracy: 0.8317 - val_loss: 0.7148 - val_accuracy: 0.8347\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 34s 5s/step - loss: 0.6844 - accuracy: 0.8376 - val_loss: 0.6801 - val_accuracy: 0.8347\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 35s 5s/step - loss: 0.6746 - accuracy: 0.8328 - val_loss: 0.6535 - val_accuracy: 0.8347\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 32s 5s/step - loss: 0.6166 - accuracy: 0.8391 - val_loss: 0.6286 - val_accuracy: 0.8347\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 31s 4s/step - loss: 0.5925 - accuracy: 0.8353 - val_loss: 0.6045 - val_accuracy: 0.8346\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 29s 4s/step - loss: 0.5421 - accuracy: 0.8389 - val_loss: 0.5885 - val_accuracy: 0.8346\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.5280 - accuracy: 0.8366 - val_loss: 0.5831 - val_accuracy: 0.8347\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 26s 4s/step - loss: 0.4811 - accuracy: 0.8467 - val_loss: 0.5671 - val_accuracy: 0.8343\n"
     ]
    }
   ],
   "source": [
    "# fit model for 10 epoch on this sequence\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9969328513528316 0.9927461139896373 0.99483507774711\n"
     ]
    }
   ],
   "source": [
    "TP=FP=FN=TN=0\n",
    "for j in range(len(X_test)):\n",
    "    p = model.predict(np.array([X_test[j]]))\n",
    "    y_pred = np.argmax(p, axis=-1)\n",
    "    y_real=np.argmax([y_test[j]],axis=-1)\n",
    "    \n",
    "    for i in range(len(y_real[0])):\n",
    "        if y_pred[0][i]==y_real[0][i]==1:\n",
    "            TP+=1\n",
    "        elif y_real[0][i]==1 and y_real[0][i]!=y_pred[0][i]:\n",
    "            FP+=1\n",
    "        elif y_real[0][i]==y_pred[0][i]==0:\n",
    "            TN+=1\n",
    "        elif y_pred[0][i]==0 and y_real[0][i]!=y_pred[0][i]:\n",
    "            FN+=1\n",
    "\n",
    "precision=(TP/(TP+FP))\n",
    "recall=(TP/(TP+FN))\n",
    "f1_score= (2*precision*recall)/(precision + recall)\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word   Pred\n",
      "Similar       : O\n",
      "numerical     : I-Process\n",
      "oscillations  : I-Task\n",
      "to            : I-Task\n",
      "those         : I-Task\n",
      "described     : I-Process\n",
      "above         : O\n",
      "also          : O\n",
      "emerge        : O\n",
      "in            : O\n",
      "the           : O\n",
      "ISPM          : O\n",
      "when          : O\n",
      "utilising     : O\n",
      "classical     : O\n",
      "IBM           : O\n",
      "kernels       : O\n",
      "due           : I-Process\n",
      "to            : O\n",
      "their         : O\n",
      "lack          : O\n",
      "of            : O\n",
      "regularity    : O\n",
      "(             : O\n",
      "with          : O\n",
      "discontinuous : O\n",
      "second        : O\n",
      "derivatives   : O\n",
      ")             : O\n",
      ".             : O\n",
      "Furthermore   : O\n",
      ",             : O\n",
      "it            : O\n",
      "is            : O\n",
      "important     : O\n",
      "to            : O\n",
      "remark        : O\n",
      "that          : O\n",
      "the           : O\n",
      "immersed      : O\n",
      "structure     : O\n",
      "stresses      : O\n",
      "are           : O\n",
      "captured      : O\n",
      "in            : O\n",
      "the           : O\n",
      "Lagrangian    : O\n",
      "description   : O\n",
      "and           : O\n",
      "hence         : O\n",
      ",             : O\n",
      "in            : O\n",
      "order         : O\n",
      "to            : O\n",
      "compute       : O\n",
      "them          : O\n",
      "accurately    : O\n",
      ",             : O\n",
      "it            : O\n",
      "is            : O\n",
      "important     : O\n",
      "to            : O\n",
      "ensure        : O\n",
      "that          : O\n",
      "these         : O\n",
      "spurious      : O\n",
      "oscillations  : O\n",
      "are           : O\n",
      "not           : O\n",
      "introduced    : O\n",
      "via           : O\n",
      "the           : O\n",
      "kernel        : O\n",
      "interpolation : O\n",
      "functions.    : O\n",
      "In            : O\n",
      "this          : I-Process\n",
      "paper         : O\n",
      ",             : O\n",
      "the           : O\n",
      "authors       : I-Process\n",
      "have          : O\n",
      "specifically  : O\n",
      "designed      : O\n",
      "a             : O\n",
      "new           : O\n",
      "family        : O\n",
      "of            : O\n",
      "kernel        : O\n",
      "functions     : O\n",
      "which         : O\n",
      "do            : O\n",
      "not           : O\n",
      "introduce     : O\n",
      "these         : O\n",
      "spurious      : O\n",
      "oscillations. : O\n",
      "The           : O\n",
      "kernel        : O\n",
      "functions     : O\n",
      "are           : O\n",
      "obtained      : O\n",
      "by            : O\n",
      "taking        : O\n",
      "into          : O\n",
      "account       : O\n",
      "discrete      : O\n",
      "reproducibility: O\n",
      "conditions    : O\n",
      "as            : O\n",
      "originally    : O\n",
      "introduced    : O\n",
      "by            : O\n",
      "Peskin        : O\n",
      "[             : O\n",
      "14            : O\n",
      "]             : O\n",
      "(             : O\n",
      "in            : O\n",
      "our           : O\n",
      "case          : O\n",
      ",             : O\n",
      "tailor-made   : O\n",
      "for           : O\n",
      "Cartesian     : O\n",
      "staggered     : O\n",
      "grids         : O\n",
      ")             : O\n",
      "and           : O\n",
      "regularity    : O\n",
      "requirements  : O\n",
      "to            : O\n",
      "prevent       : O\n",
      "the           : O\n",
      "appearance    : O\n",
      "of            : O\n",
      "spurious      : O\n",
      "oscillations  : O\n",
      "when          : O\n",
      "computing     : O\n",
      "derivatives.  : O\n",
      "A             : O\n",
      "Maple         : O\n",
      "computer      : O\n",
      "program       : O\n",
      "has           : O\n",
      "been          : O\n",
      "developed     : O\n",
      "to            : O\n",
      "obtain        : O\n",
      "explicit      : O\n",
      "expressions   : O\n",
      "for           : O\n",
      "the           : O\n",
      "new           : O\n",
      "kernels       : O\n",
      ".             : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : I-Process\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n",
      "unanswered.   : O\n"
     ]
    }
   ],
   "source": [
    "i=random.randint(0,len(X_test))\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)  \n",
    "print(\"{}   {}\".format(\"Word\", \"Pred\"))\n",
    "for w,pred in zip(X_test[0],p[0]):\n",
    "    print(\"{:14}: {}\".format(words[w],tags[pred]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
